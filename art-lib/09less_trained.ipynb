{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks import FastGradientMethod\n",
    "from art.attacks import DeepFool\n",
    "from art.attacks import SaliencyMapMethod\n",
    "from art.attacks import ProjectedGradientDescent\n",
    "from art.classifiers import PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # mnist의 경우 28*28의 흑백이미지(input channel=1)이다.\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 14*14가 된다\n",
    "        # 첫번재 convolution layer에서 나온 output channel이 32이므로 2번째 input도 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 7*7이 된다\n",
    "        # fc -> fully connected, fc는 모든 weight를 고려해서 만들기 때문에 cnn에서는 locally connected를 이용하여 만든다.\n",
    "        # nn.Linear에서는 conv를 거친 feature map을 1차원으로 전부 바꿔서 input을 한다. 이게 64*7*7\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7) # linear에 들어갈 수 있도록 reshape\n",
    "        x = F.relu(self.fc1(x)) # fully connected에 relu 적용\n",
    "        x = F.dropout(x, training=self.training) # 가중치 감소만으로는 overfit을 해결하기가 어려움, 그래서 뉴런의 연결을 임의로 삭제\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1, 3).astype(np.float32)\n",
    "x_test = np.swapaxes(x_test, 1, 3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available!\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "if is_cuda: print(\"CUDA available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undefended model loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "model_path = './model/mnist_um_art.pth'\n",
    "model = MnistModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "models.append(model)\n",
    "\n",
    "print(\"undefended model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linf trained model1 loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = './model/mnist_fgsm_art.pth'\n",
    "model = MnistModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "models.append(model)\n",
    "\n",
    "print(\"Linf trained model1 loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained 6 model loaded\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    model = MnistModel().to(device)\n",
    "    models.append(model)\n",
    "\n",
    "print(\"untrained 6 model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the ART classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifiers created\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 0-um, 1-fgsm\n",
    "classifiers = []\n",
    "for model in models:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    classifier = PyTorchClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), loss=criterion,\n",
    "                               optimizer=optimizer, input_shape=(1, 28, 28), nb_classes=10)\n",
    "    classifiers.append(classifier)\n",
    "    \n",
    "print(\"classifiers created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM example generated\n"
     ]
    }
   ],
   "source": [
    "adv_tests = []\n",
    "clean = classifiers[0]\n",
    "\n",
    "Linf1_attack = FastGradientMethod(classifier=clean, eps=0.3)\n",
    "Linf1_x_test_adv = Linf1_attack.generate(x=x_test)\n",
    "adv_tests.append(Linf1_x_test_adv)\n",
    "\n",
    "print('FGSM example generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD example generated\n"
     ]
    }
   ],
   "source": [
    "Linf2_attack = ProjectedGradientDescent(classifier=clean, eps=0.3)\n",
    "Linf2_x_test_adv = Linf2_attack.generate(x=x_test)\n",
    "adv_tests.append(Linf2_x_test_adv)\n",
    "\n",
    "print('PGD example generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train new type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier2 with benign example...\n",
      "Training done!\n",
      "\n",
      "Training classifier3 with benign example...\n",
      "Training done!\n",
      "\n",
      "Training classifier4 with benign example...\n",
      "Training done!\n",
      "\n",
      "Training classifier5 with benign example...\n",
      "Training done!\n",
      "\n",
      "Training classifier6 with benign example...\n",
      "Training done!\n",
      "\n",
      "Training classifier7 with benign example...\n",
      "Training done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(\"Training classifier{} with benign example...\".format(i+2))\n",
    "    idx = 1 + i\n",
    "    classifiers[i+2].fit(x_train, y_train, batch_size=64, nb_epochs=idx*5)\n",
    "    print('Training done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate each models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model #: 0-um, 1-fgsm, 2~7-new\n",
      "test  #: 0-fgsm, 2-pgd\n",
      "\n",
      "model #0\n",
      "Accuracy on benign test examples: 98.99%\n",
      "Accuracy on model with adversarial test #0 examples: 3.01%\n",
      "Accuracy on model with adversarial test #1 examples: 0.74%\n",
      "\n",
      "model #1\n",
      "Accuracy on benign test examples: 88.83%\n",
      "Accuracy on model with adversarial test #0 examples: 98.3%\n",
      "Accuracy on model with adversarial test #1 examples: 86.64%\n",
      "\n",
      "model #2\n",
      "Accuracy on benign test examples: 98.2%\n",
      "Accuracy on model with adversarial test #0 examples: 12.479999999999999%\n",
      "Accuracy on model with adversarial test #1 examples: 3.25%\n",
      "\n",
      "model #3\n",
      "Accuracy on benign test examples: 98.47%\n",
      "Accuracy on model with adversarial test #0 examples: 6.5600000000000005%\n",
      "Accuracy on model with adversarial test #1 examples: 1.1400000000000001%\n",
      "\n",
      "model #4\n",
      "Accuracy on benign test examples: 98.94%\n",
      "Accuracy on model with adversarial test #0 examples: 5.6000000000000005%\n",
      "Accuracy on model with adversarial test #1 examples: 0.77%\n",
      "\n",
      "model #5\n",
      "Accuracy on benign test examples: 99.0%\n",
      "Accuracy on model with adversarial test #0 examples: 5.06%\n",
      "Accuracy on model with adversarial test #1 examples: 0.8999999999999999%\n",
      "\n",
      "model #6\n",
      "Accuracy on benign test examples: 98.96000000000001%\n",
      "Accuracy on model with adversarial test #0 examples: 6.12%\n",
      "Accuracy on model with adversarial test #1 examples: 0.89%\n",
      "\n",
      "model #7\n",
      "Accuracy on benign test examples: 99.11%\n",
      "Accuracy on model with adversarial test #0 examples: 5.29%\n",
      "Accuracy on model with adversarial test #1 examples: 0.8099999999999999%\n"
     ]
    }
   ],
   "source": [
    "print('model #: 0-um, 1-fgsm, 2~7-new')\n",
    "print('test  #: 0-fgsm, 2-pgd')\n",
    "\n",
    "models_accuracy = []\n",
    "# test order: 0-fgsm\n",
    "for i, (classifier) in enumerate(classifiers):\n",
    "    print('\\nmodel #{}'.format(i))\n",
    "    model_accuracy = []\n",
    "    predictions = classifier.predict(x_test)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "    print('Accuracy on benign test examples: {}%'.format(accuracy * 100))\n",
    "    model_accuracy.append(accuracy)\n",
    "    for j, (x_test_adv) in enumerate(adv_tests):\n",
    "        predictions = classifier.predict(x_test_adv)\n",
    "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "        print('Accuracy on model with adversarial test #{} examples: {}%'.format(j, accuracy * 100))\n",
    "        model_accuracy.append(accuracy)\n",
    "    models_accuracy.append(model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undefended</th>\n",
       "      <th>Linf1</th>\n",
       "      <th>new1</th>\n",
       "      <th>new5</th>\n",
       "      <th>new10</th>\n",
       "      <th>new15</th>\n",
       "      <th>new20</th>\n",
       "      <th>new25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Benign</th>\n",
       "      <td>0.9899</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGSM</th>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>0.0529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PGD</th>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        undefended   Linf1    new1    new5   new10   new15   new20   new25\n",
       "Benign      0.9899  0.8883  0.9820  0.9847  0.9894  0.9900  0.9896  0.9911\n",
       "FGSM        0.0301  0.9830  0.1248  0.0656  0.0560  0.0506  0.0612  0.0529\n",
       "PGD         0.0074  0.8664  0.0325  0.0114  0.0077  0.0090  0.0089  0.0081"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "data = {\n",
    "    'undefended': models_accuracy[0],\n",
    "    'Linf1': models_accuracy[1],\n",
    "    'new1': models_accuracy[2],\n",
    "    'new5': models_accuracy[3],\n",
    "    'new10': models_accuracy[4],\n",
    "    'new15': models_accuracy[5],\n",
    "    'new20': models_accuracy[6],\n",
    "    'new25': models_accuracy[7],\n",
    "}\n",
    "\n",
    "columns = ['undefended', 'Linf1', 'new1', 'new5', 'new10', 'new15', 'new20', 'new25']\n",
    "idx = ['Benign','FGSM', 'PGD']\n",
    "DataFrame(data, columns=columns, index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

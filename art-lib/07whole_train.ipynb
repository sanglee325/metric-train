{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks import FastGradientMethod\n",
    "from art.attacks import DeepFool\n",
    "from art.attacks import SaliencyMapMethod\n",
    "from art.attacks import ProjectedGradientDescent\n",
    "from art.classifiers import PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # mnist의 경우 28*28의 흑백이미지(input channel=1)이다.\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 14*14가 된다\n",
    "        # 첫번재 convolution layer에서 나온 output channel이 32이므로 2번째 input도 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 7*7이 된다\n",
    "        # fc -> fully connected, fc는 모든 weight를 고려해서 만들기 때문에 cnn에서는 locally connected를 이용하여 만든다.\n",
    "        # nn.Linear에서는 conv를 거친 feature map을 1차원으로 전부 바꿔서 input을 한다. 이게 64*7*7\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7) # linear에 들어갈 수 있도록 reshape\n",
    "        x = F.relu(self.fc1(x)) # fully connected에 relu 적용\n",
    "        x = F.dropout(x, training=self.training) # 가중치 감소만으로는 overfit을 해결하기가 어려움, 그래서 뉴런의 연결을 임의로 삭제\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()\n",
    "\n",
    "x_train = np.swapaxes(x_train, 1, 3).astype(np.float32)\n",
    "x_test = np.swapaxes(x_test, 1, 3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available!\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "if is_cuda: print(\"CUDA available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clean models loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "model_path = './model/mnist_um_art.pth'\n",
    "model = MnistModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "models.append(model)\n",
    "\n",
    "model_path = './model/mnist_um_art.pth'\n",
    "model = MnistModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "models.append(model)\n",
    "\n",
    "print(\"2 clean models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the ART classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifiers created\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 0-um, 1-trained\n",
    "classifiers = []\n",
    "for model in models:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    classifier = PyTorchClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), loss=criterion,\n",
    "                               optimizer=optimizer, input_shape=(1, 28, 28), nb_classes=10)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "print(\"classifiers created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate adversarial test and train examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_tests = []\n",
    "adv_trains = []\n",
    "trained = classifiers[1]\n",
    "\n",
    "\n",
    "Linf1_attack = FastGradientMethod(classifier=trained, eps=0.3)\n",
    "\n",
    "Linf1_x_test_adv = Linf1_attack.generate(x=x_test)\n",
    "adv_tests.append(Linf1_x_test_adv)\n",
    "\n",
    "Linf1_x_train_adv = Linf1_attack.generate(x=x_train)\n",
    "adv_trains.append(Linf1_x_train_adv)\n",
    "\n",
    "print('FGSM example generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linf2_attack = ProjectedGradientDescent(classifier=trained, eps=0.3)\n",
    "\n",
    "Linf2_x_test_adv = Linf2_attack.generate(x=x_test)\n",
    "adv_tests.append(Linf2_x_test_adv)\n",
    "\n",
    "Linf2_x_train_adv = Linf2_attack.generate(x=x_train)\n",
    "adv_trains.append(Linf2_x_train_adv)\n",
    "\n",
    "print('PGD example generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L0_attack = SaliencyMapMethod(classifier=trained)\n",
    "\n",
    "L0_x_test_adv = L0_attack.generate(x=x_test)\n",
    "adv_tests.append(L0_x_test_adv)\n",
    "\n",
    "L0_x_train_adv = L0_attack.generate(x=x_train)\n",
    "adv_trains.append(L0_x_train_adv)\n",
    "\n",
    "print('JSMA example generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_attack = DeepFool(classifier=trained)\n",
    "\n",
    "L2_x_test_adv = L2_attack.generate(x=x_test)\n",
    "adv_tests.append(L2_x_test_adv)\n",
    "\n",
    "L2_x_train_adv = L2_attack.generate(x=x_train)\n",
    "adv_trains.append(L2_x_train_adv)\n",
    "\n",
    "print('Deepfool example generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with adversarial example...\\n\")\n",
    "\n",
    "for i, (x_train_adv) in enumerate(adv_trains):\n",
    "    classifier[1].fit(x_train_adv, y_train, batch_size=64, nb_epochs=30)\n",
    "    print('Train {} done'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate model on adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_accuracy = []\n",
    "\n",
    "print('model #: 0-um, 1-trained')\n",
    "print('test #: 0-fgsm, 1-pgd, 2-jsma, 3-deepfool')\n",
    "\n",
    "# test order: 0-fgsm, 1-pgd, 2-jsma, 3-deepfool\n",
    "for i, (classifier) in enumerate(classifiers):\n",
    "    print('\\nmodel #{}'.format(i))\n",
    "    model_accuracy = []\n",
    "    predictions = classifier.predict(x_test)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "    print('Accuracy on benign test examples: {}%'.format(accuracy * 100))\n",
    "    model_accuracy.append(accuracy)\n",
    "    for j, (x_test_adv) in enumerate(adv_attacks):\n",
    "        predictions = classifier.predict(x_test_adv)\n",
    "        accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "        print('Accuracy on model with adversarial test #{} examples: {}%'.format(j, accuracy * 100))\n",
    "        model_accuracy.append(accuracy)\n",
    "    models_accuracy.append(model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "data = {\n",
    "    'clean': models_accuracy[0],\n",
    "    'trained': models_accuracy[1],\n",
    "}\n",
    "\n",
    "columns = ['clean', 'trained']\n",
    "idx = ['Benign' ,'FGSM', 'PGD', 'JSMA', 'Deepfool']\n",
    "DataFrame(data, columns=columns, index=idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

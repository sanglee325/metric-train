{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # mnist의 경우 28*28의 흑백이미지(input channel=1)이다.\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 14*14가 된다\n",
    "        # 첫번재 convolution layer에서 나온 output channel이 32이므로 2번째 input도 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 7*7이 된다\n",
    "        # fc -> fully connected, fc는 모든 weight를 고려해서 만들기 때문에 cnn에서는 locally connected를 이용하여 만든다.\n",
    "        # nn.Linear에서는 conv를 거친 feature map을 1차원으로 전부 바꿔서 input을 한다. 이게 64*7*7\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7) # linear에 들어갈 수 있도록 reshape\n",
    "        x = F.relu(self.fc1(x)) # fully connected에 relu 적용\n",
    "        x = F.dropout(x, training=self.training) # 가중치 감소만으로는 overfit을 해결하기가 어려움, 그래서 뉴런의 연결을 임의로 삭제\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deepfool attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool_attack(model, device, test_loader):\n",
    "    print(\"Attack Image!!\")\n",
    "    \n",
    "    perturbed_images = []\n",
    "    for image, label in test_loader:\n",
    "        attack = foolbox.attacks.DeepFoolAttack(model)\n",
    "        image_np = attack(image.numpy(), label.numpy())\n",
    "        perturbed_image = torch.from_numpy(image_np)\n",
    "        perturbed_images.append((perturbed_image, label))\n",
    "\n",
    "    print(\"Attack Done!!\")\n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, images):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # test set의 모든 예제를 test한다\n",
    "    for image, label in images:\n",
    "        # cpu나 gpu로 데이터를 전송한다\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(image)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == label.item():\n",
    "            correct += 1\n",
    "            if (len(adv_examples) < 5):\n",
    "                adv_ex = image.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (label.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = image.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (label.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # final_acc = correct/idx\n",
    "    final_acc = correct/float(len(images))\n",
    "    print(\"Test Accuracy = {} / {} = {}\".format(correct, len(images), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load dataset and pretrained model, Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])),\n",
    "        batch_size=1, shuffle=True)\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "download_path = './data'\n",
    "train_dataset = MNIST(download_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset = MNIST(download_path, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "print(\"CUDA Available:\", is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pksll325/anaconda3/envs/pytorch/lib/python3.6/site-packages/foolbox/models/pytorch.py:71: UserWarning: The PyTorch model is in training mode and therefore might not be deterministic. Call the eval() method to set it in evaluation mode if this is not intended.\n",
      "  \"The PyTorch model is in training mode and therefore might\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained_model: 이전에 training한 mnist 모델\n",
    "pretrained_model = './model/mnist_um.pth'\n",
    "df_defense_model = './model/mnist_deepfool_model.pth'\n",
    "\n",
    "model_normal = MnistModel().to(device)\n",
    "model_normal.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "fmodel = foolbox.models.PyTorchModel(model_normal, bounds=(0, 1), num_classes=10)\n",
    "\n",
    "model_df = MnistModel().to(device)\n",
    "model_df.load_state_dict(torch.load(df_defense_model, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Image!!\n",
      "Attack Done!!\n"
     ]
    }
   ],
   "source": [
    "perturbed_images = deepfool_attack(fmodel, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal:\n",
      "Test Accuracy = 3501 / 10000 = 0.3501\n"
     ]
    }
   ],
   "source": [
    "normal_accuracies = []\n",
    "examples = []\n",
    "\n",
    "\n",
    "print('Normal:')\n",
    "acc, ex = test(model_normal, device, perturbed_images)\n",
    "normal_accuracies.append(acc)\n",
    "examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepfool defense:\n",
      "Test Accuracy = 4980 / 10000 = 0.498\n"
     ]
    }
   ],
   "source": [
    "df_accuracies = []\n",
    "\n",
    "print('Deepfool defense:')    \n",
    "acc, ex = test(model_df, device, perturbed_images)\n",
    "df_accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,5))\n",
    "plt.bar(['UM'], normal_accuracies, label='UM', color='b')\n",
    "plt.bar(['Deepfool'], df_accuracies, label='DF', color='g')\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.title(\"Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "for idx, (label, pred, image) in enumerate(examples[0]):\n",
    "    #image = image.numpy()\n",
    "    title = str(idx) + '_' + str(label) + '_' + str(pred) + '.png'\n",
    "    image_t = torch.from_numpy(image)\n",
    "    save_image(image_t, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

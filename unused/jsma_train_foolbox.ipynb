{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # mnist의 경우 28*28의 흑백이미지(input channel=1)이다.\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 14*14가 된다\n",
    "        # 첫번재 convolution layer에서 나온 output channel이 32이므로 2번째 input도 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5, padding=2)\n",
    "        # feature map의 크기는 7*7이 된다\n",
    "        # fc -> fully connected, fc는 모든 weight를 고려해서 만들기 때문에 cnn에서는 locally connected를 이용하여 만든다.\n",
    "        # nn.Linear에서는 conv를 거친 feature map을 1차원으로 전부 바꿔서 input을 한다. 이게 64*7*7\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7) # linear에 들어갈 수 있도록 reshape\n",
    "        x = F.relu(self.fc1(x)) # fully connected에 relu 적용\n",
    "        x = F.dropout(x, training=self.training) # 가중치 감소만으로는 overfit을 해결하기가 어려움, 그래서 뉴런의 연결을 임의로 삭제\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advtrain(model, device, train_loader, optimizer, epoch, log_interval, train_loader_origin):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    # in training loop:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # gpu나 cpu로 데이터를 옮김\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # gradient descent전에 gradient buffer를 0으로 초기화 하는 역할을 한다\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # negative log-likelihood: nll loss, 딥러닝 모델의 손실함수이다\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update based on the current gradient stored in grad\n",
    "        # 여기서 기존에 저장되어있던 gradient를 기반으로 update 하기 때문에 위의 초기화가 필요함\n",
    "        avg_loss+=F.nll_loss(output, target, reduction='sum').item()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_origin.dataset),\n",
    "                100. * batch_idx / len(train_loader_origin), loss.item()))\n",
    "    avg_loss/=len(train_loader_origin.dataset)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, images):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    failed_examples = []\n",
    "    correct_examples = []\n",
    "\n",
    "    # test set의 모든 예제를 test한다\n",
    "    for image, label in images:\n",
    "        # cpu나 gpu로 데이터를 전송한다\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(image)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == label.item():\n",
    "            correct += 1\n",
    "            if (len(correct_examples) < 5):\n",
    "                adv_ex = image.squeeze().detach().cpu().numpy()\n",
    "                correct_examples.append( (label.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            if len(failed_examples) < 5:\n",
    "                adv_ex = image.squeeze().detach().cpu().numpy()\n",
    "                failed_examples.append( (label.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # final_acc = correct/idx\n",
    "    final_acc = correct/float(len(images))\n",
    "    print(\"Test Accuracy = {} / {} = {}\\n\".format(correct, len(images), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, correct_examples, failed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=64\n",
    "test_batch_size=1\n",
    "epochs=50\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 1000\n",
    "\n",
    "target_class=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = './model/mnist_um.pth'\n",
    "model = MnistModel().to(device).eval()\n",
    "#model.load_state_dict(torch.load(pretrained_model, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum: 기울기에서 속도의 개념을 추가, 기울기 업데이트시 폭을 조절한다\n",
    "# lr: learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# mean=0.5 std=1.0으로 Normalize한다\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = './data'\n",
    "train_dataset = MNIST(download_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset = MNIST(download_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=train_batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from foolbox.criteria import TargetClass\n",
    "\n",
    "def jsma_attack(model, loader, target_class):\n",
    "    print(\"Attack image!\")\n",
    "    \n",
    "    #criterion = TargetClass(target_class)\n",
    "    perturbed_images = []\n",
    "    for idx, (image, label) in enumerate(loader):\n",
    "        #attack = foolbox.attacks.SaliencyMapAttack(model, criterion)\n",
    "        attack = foolbox.attacks.SaliencyMapAttack(model)\n",
    "        image_np = attack(image.numpy(), label.numpy())\n",
    "        perturbed_image = torch.from_numpy(image_np)\n",
    "        perturbed_images.append((perturbed_image, label))\n",
    "        \n",
    "    print(\"Attack done!\")\n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack image!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pksll325/anaconda3/envs/pytorch/lib/python3.6/site-packages/foolbox/attacks/base.py:95: UserWarning: GradientAttack did not find an adversarial, maybe the model or the criterion is not supported by this attack.\n",
      "  \" attack.\".format(self.name())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack done!\n"
     ]
    }
   ],
   "source": [
    "perturbed_test = jsma_attack(fmodel, test_loader, target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack image!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pksll325/anaconda3/envs/pytorch/lib/python3.6/site-packages/foolbox/attacks/base.py:95: UserWarning: SaliencyMapAttack did not find an adversarial, maybe the model or the criterion is not supported by this attack.\n",
      "  \" attack.\".format(self.name())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack done!\n"
     ]
    }
   ],
   "source": [
    "perturbed_train = jsma_attack(fmodel, train_loader, target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309126\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: nan\n",
      "Test Accuracy = 980 / 10000 = 0.098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    trn_loss = advtrain(model, device, perturbed_train, optimizer, epoch, log_interval, train_loader)\n",
    "    train_losses.append(trn_loss)\n",
    "    accuracy, L2_correct, L2_failed = test(model, device, perturbed_test)\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './model/x_mnist_jsma_foolbox.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pf_images(model, device, perturbed_data):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    failed_image = []\n",
    "    correct_image = []\n",
    "    \n",
    "    for idx, (data, target) in enumerate(perturbed_data):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "        # dim 1에서 가장 큰 값을 가지는 것의 index를 반환\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        final_pred = output.max(1, keepdim=True)[1]\n",
    "        # pred.eq(data)는 pred와 data 배열이 일치하는지를 검사\n",
    "        # target을 pred처럼 보이게 만들어서 .sum()을 이용해 일치하는 것들의 개수를 구한다\n",
    "        # .item()은 tensor의 값을 스칼라로 만드는 역할을 한다\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        for j in pred.eq(target.view_as(pred)):\n",
    "            if j == 1:\n",
    "                correct_image.append((data, target.item(), final_pred.item()))\n",
    "            else:\n",
    "                failed_image.append((data, target.item(), final_pred.item()))\n",
    "    return correct_image, failed_image\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "correct, failed = get_pf_images(model, device, perturbed_test)\n",
    "SAVE_PATH = './examples/jsma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (image, label, pred) in enumerate(failed):\n",
    "    if idx < 10:\n",
    "        title = SAVE_PATH + 'fail/' + str(idx) +'_'+ str(label) + '_' + str(pred) +'.png'\n",
    "        save_image(image, title)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (image, label, pred) in enumerate(correct):\n",
    "    if idx < 10:\n",
    "        title = SAVE_PATH + 'correct/' + str(idx) +'_'+ str(label) + '_' + str(pred) +'.png'\n",
    "        save_image(image, title)\n",
    "    else: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
